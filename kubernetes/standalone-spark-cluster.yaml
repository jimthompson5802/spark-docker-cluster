apiVersion: v1
items:

###
#  Service definitions
###
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kompose.cmd: kompose convert -f resolved-docker-compose.yaml --volumes hostPath
        -o kubernetes-spark-cluster.yaml
      kompose.version: 1.18.0 ()
    creationTimestamp: null
    labels:
      io.kompose.service: pyspnb-client
    name: pyspnb-client
  spec:
    ports:
    - name: "8888"
      port: 8888
      targetPort: 8888
    - name: "4040"
      port: 4040
      targetPort: 4040
    - name: "4041"
      port: 4041
      targetPort: 4041
    selector:
      io.kompose.service: pyspnb-client
  status:
    loadBalancer: {}

- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kompose.cmd: kompose convert -f resolved-docker-compose.yaml
      kompose.version: 1.18.0 ()
    creationTimestamp: null
    labels:
      io.kompose.service: spark-hdfs
    name: spark-hdfs
  spec:
    ports:
    - name: "8080"
      port: 8080
      targetPort: 8080
    selector:
      io.kompose.service: spark-hdfs
  status:
    loadBalancer: {}

- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kompose.cmd: kompose convert -f resolved-docker-compose.yaml
      kompose.version: 1.18.0 ()
    creationTimestamp: null
    labels:
      io.kompose.service: spark-worker1
    name: spark-worker1
  spec:
    ports:
    - name: "18081"
      port: 18081
      targetPort: 18081
    selector:
      io.kompose.service: spark-worker1
  status:
    loadBalancer: {}

- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kompose.cmd: kompose convert -f resolved-docker-compose.yaml
      kompose.version: 1.18.0 ()
    creationTimestamp: null
    labels:
      io.kompose.service: spark-worker2
    name: spark-worker2
  spec:
    ports:
    - name: "28081"
      port: 28081
      targetPort: 28081
    selector:
      io.kompose.service: spark-worker2
  status:
    loadBalancer: {}


###
# Deployment Definitions
###
- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      kompose.cmd: kompose convert -f resolved-docker-compose.yaml --volumes hostPath
        -o kubernetes-spark-cluster.yaml
      kompose.version: 1.18.0 ()
    creationTimestamp: null
    labels:
      io.kompose.service: pyspnb-client
    name: pyspnb-client
  spec:
    replicas: 1
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          io.kompose.service: pyspnb-client
      spec:
        containers:
        - args:
          - /spark/start-pyspnb.sh
          image: spark-pyspnb:2.4.1
          name: pyspnb-client
          ports:
          - containerPort: 8888
          - containerPort: 4040
          - containerPort: 4041
          resources: {}
          volumeMounts:
          - mountPath: /opt/project
            name: pyspnb-client-hostpath0
          - mountPath: /spark/data
            name: pyspnb-client-hostpath1
        hostname: pyspnb-client
        restartPolicy: Always
        volumes:
        - name: pyspnb-client-hostpath0
          persistentVolumeClaim:
            claimName: project-dir-claim
        - name: pyspnb-client-hostpath1
          persistentVolumeClaim:
            claimName: spark-data-claim
  status: {}

- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      kompose.cmd: kompose convert -f resolved-docker-compose.yaml
      kompose.version: 1.18.0 ()
    creationTimestamp: null
    labels:
      io.kompose.service: spark-hdfs
    name: spark-hdfs
  spec:
    replicas: 1
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          io.kompose.service: spark-hdfs
      spec:
        containers:
        - args:
          - /spark/start-spark-master.sh
          image: spark-master:2.4.1
          name: spark-hdfs
          ports:
          - containerPort: 8080
          resources: {}
          volumeMounts:
          - mountPath: /spark/data
            name: spark-master-claim0
        hostname: spark-hdfs
        restartPolicy: Always
        volumes:
        - name: spark-master-claim0
          persistentVolumeClaim:
            claimName: spark-data-claim
  status: {}

- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      kompose.cmd: kompose convert -f resolved-docker-compose.yaml
      kompose.version: 1.18.0 ()
    creationTimestamp: null
    labels:
      io.kompose.service: spark-worker1
    name: spark-worker1
  spec:
    replicas: 1
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          io.kompose.service: spark-worker1
      spec:
        containers:
        - args:
          - /spark/start-spark-worker.sh
          - --cores
          - "4"
          - --memory
          - 4g
          - --webui-port
          - "18081"
          image: spark-worker:2.4.1
          name: spark-worker1
          ports:
          - containerPort: 18081
          resources: {}
          volumeMounts:
          - mountPath: /spark/data
            name: spark-worker1-claim0
        hostname: spark-worker1
        restartPolicy: Always
        volumes:
        - name: spark-worker1-claim0
          persistentVolumeClaim:
            claimName: spark-data-claim
  status: {}

- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    annotations:
      kompose.cmd: kompose convert -f resolved-docker-compose.yaml
      kompose.version: 1.18.0 ()
    creationTimestamp: null
    labels:
      io.kompose.service: spark-worker2
    name: spark-worker2
  spec:
    replicas: 1
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          io.kompose.service: spark-worker2
      spec:
        containers:
        - args:
          - /spark/start-spark-worker.sh
          - --cores
          - "4"
          - --memory
          - 4g
          - --webui-port
          - "28081"
          image: spark-worker:2.4.1
          name: spark-worker2
          ports:
          - containerPort: 28081
          resources: {}
          volumeMounts:
          - mountPath: /spark/data
            name: spark-worker2-claim0
        hostname: spark-worker2
        restartPolicy: Always
        volumes:
        - name: spark-worker2-claim0
          persistentVolumeClaim:
            claimName: spark-data-claim
  status: {}



###
# Persistent Volume definitions
###

# Volume containing project source code
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: project-pv-volume
    labels:
      type: local
  spec:
    storageClassName: project-dir
    capacity:
      storage: 10Gi
    accessModes:
      - ReadWriteOnce
    hostPath:
      path: "/Users/jim/Desktop/docker_sandbox/spark-docker-cluster/project"

# Volume simulating distributed file system
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: data-pv-volume
    labels:
      type: local
  spec:
    storageClassName: spark-data
    capacity:
      storage: 10Gi
    accessModes:
      - ReadWriteOnce
    hostPath:
      path: "/Users/jim/Desktop/docker_sandbox/spark-docker-cluster/data"


###
# Persistent Volume Claims
###
- apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    creationTimestamp: null
    labels:
      io.kompose.service: pyspnb-client-claim0
    name: project-dir-claim
  spec:
    storageClassName: project-dir
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 10Gi
  status: {}

- apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    creationTimestamp: null
    labels:
      io.kompose.service: pyspnb-client-claim1
    name: spark-data-claim
  spec:
    storageClassName: spark-data
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 10Gi
  status: {}

kind: List
metadata: {}

