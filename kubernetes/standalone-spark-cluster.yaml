
###
#  pyspnb-client
###
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: pyspnb-client
  name: pyspnb-client
spec:
  ports:
  - name: "8888"
    port: 8888
    targetPort: 8888
  - name: "4040"
    port: 4040
    targetPort: 4040
  selector:
    app: pyspnb-client
  type: LoadBalancer

---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: pyspnb-client
  name: pyspnb-client
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: pyspnb-client
    spec:
      containers:
      - args:
        - /spark/start-pyspnb.sh
        image: spark-pyspnb:2.4.1
        name: pyspnb-client
        ports:
        - containerPort: 8888
        - containerPort: 4040
        resources: {}
        volumeMounts:
        - mountPath: /opt/project
          name: pyspnb-client-hostpath0
        - mountPath: /spark/data
          name: pyspnb-client-hostpath1
      hostname: pyspnb-client
      restartPolicy: Always
      volumes:
      - name: pyspnb-client-hostpath0
        persistentVolumeClaim:
          claimName: project-dir-claim
      - name: pyspnb-client-hostpath1
        persistentVolumeClaim:
          claimName: spark-data-claim


###
# Spark Master
###
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: spark-master
  name: spark-master-kub
spec:
  ports:
  - name: "8080"
    port: 8080
    targetPort: 8080
  selector:
    app: spark-master
  clusterIP: None

---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: spark-master
  name: spark-master
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: spark-master
    spec:
      containers:
      - args:
        - /spark/start-spark-master.sh
        image: spark-master:2.4.1
        name: spark-master
        ports:
        - containerPort: 8080
        resources: {}
        volumeMounts:
        - mountPath: /spark/data
          name: spark-master-claim0
      hostname: spark-master
      restartPolicy: Always
      volumes:
      - name: spark-master-claim0
        persistentVolumeClaim:
          claimName: spark-data-claim

###
# Sparker Woker1
###
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: spark-worker1
  name: spark-worker1
spec:
  ports:
  - name: "18081"
    port: 18081
    targetPort: 18081
  selector:
    app: spark-worker1
  clusterIP: None

---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: spark-worker1
  name: spark-worker1
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: spark-worker1
    spec:
      containers:
      - args:
        - /spark/start-spark-worker.sh
        - "spark://spark-master-kub:7077"
        - --cores
        - "4"
        - --memory
        - 4g
        - --webui-port
        - "18081"
        image: spark-worker:2.4.1
        name: spark-worker1
        ports:
        - containerPort: 18081
        resources: {}
        volumeMounts:
        - mountPath: /spark/data
          name: spark-worker1-claim0
      hostname: spark-worker1
      restartPolicy: Always
      volumes:
      - name: spark-worker1-claim0
        persistentVolumeClaim:
          claimName: spark-data-claim

###
# Spark Worker 2
###
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: spark-worker2
  name: spark-worker2
spec:
  ports:
  - name: "28081"
    port: 28081
    targetPort: 28081
  selector:
    app: spark-worker2
  clusterIP: None

---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    app: spark-worker2
  name: spark-worker2
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: spark-worker2
    spec:
      containers:
      - args:
        - /spark/start-spark-worker.sh
        - "spark://spark-master-kub:7077"
        - --cores
        - "4"
        - --memory
        - 4g
        - --webui-port
        - "28081"
        image: spark-worker:2.4.1
        name: spark-worker2
        ports:
        - containerPort: 28081
        resources: {}
        volumeMounts:
        - mountPath: /spark/data
          name: spark-worker2-claim0
      hostname: spark-worker2
      restartPolicy: Always
      volumes:
      - name: spark-worker2-claim0
        persistentVolumeClaim:
          claimName: spark-data-claim

###
# Persistent Volume definitions
###

# Volume containing project source code
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: project-pv-volume
  labels:
    type: local
spec:
  storageClassName: project-dir
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/Users/jim/Desktop/docker_sandbox/spark-docker-cluster/project"

# Volume simulating distributed file system
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: data-pv-volume
  labels:
    type: local
spec:
  storageClassName: spark-data
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/Users/jim/Desktop/docker_sandbox/spark-docker-cluster/data"


###
# Persistent Volume Claims
###
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  creationTimestamp: null
  labels:
    io.kompose.service: pyspnb-client-claim0
  name: project-dir-claim
spec:
  storageClassName: project-dir
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  creationTimestamp: null
  labels:
    io.kompose.service: pyspnb-client-claim1
  name: spark-data-claim
spec:
  storageClassName: spark-data
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

