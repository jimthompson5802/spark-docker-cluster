{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import namedtuple\n",
    "from pyspark.sql import SparkSession, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "NUMBER_OF_ROWS = 1000\n",
    "NUMBER_OF_COLS = 8\n",
    "NUMBER_BLOCKS = 2\n",
    "NUMBER_PARTITIONS = NUMBER_BLOCKS\n",
    "\n",
    "storage_platform = \"file:///spark/data\"\n",
    "\n",
    "\n",
    "hive_table_ddl = \"\"\"\n",
    "CREATE EXTERNAL TABLE hive_table(\n",
    "   X_1 double,\n",
    "   X_2 double,\n",
    "   X_3 double,\n",
    "   X_4 double,\n",
    "   X_5 double,\n",
    "   X_6 double,\n",
    "   X_7 double,\n",
    "   X_8 double,\n",
    "   part_id1 string,\n",
    "   part_id2 string\n",
    ")\n",
    "STORED AS parquet\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# HIVE oriented operations\n",
    "#\n",
    "def combineData(x):\n",
    "    x[0].update(x[1])\n",
    "    return x[0]\n",
    "\n",
    "def generate_some_data(seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    colnames = ['X_' + str(i + 1) for i in range(NUMBER_OF_COLS)]\n",
    "    numerics = np.round(np.random.randn(NUMBER_OF_ROWS, NUMBER_OF_COLS), 3)\n",
    "    numeric_list = numerics.tolist()\n",
    "    numeric_data = [dict(zip(colnames, a_row)) for a_row in numeric_list]\n",
    "\n",
    "    chr_list = zip([str(x) for x in np.random.choice(list('abcde'), NUMBER_OF_ROWS)],\n",
    "                   [str(x) for x in np.random.choice(list('xyz'), NUMBER_OF_ROWS)])\n",
    "    chrnames = ['part_id1', 'part_id2']\n",
    "    chr_data = [dict(zip(chrnames, a_row)) for a_row in chr_list]\n",
    "\n",
    "    return [Row(**kw) for kw in map(combineData, zip(numeric_data, chr_data))]\n",
    "\n",
    "my_rdd = sc.parallelize(range(NUMBER_BLOCKS), NUMBER_PARTITIONS).flatMap(generate_some_data)\n",
    "my_df = spark.createDataFrame(my_rdd)\n",
    "\n",
    "my_df.write.parquet(os.path.join(storage_platform, 'my_data_parquet'),\n",
    "                    mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+------+------+------+--------+--------+\n",
      "|   X_1|   X_2|   X_3|   X_4|   X_5|   X_6|   X_7|   X_8|part_id1|part_id2|\n",
      "+------+------+------+------+------+------+------+------+--------+--------+\n",
      "| 1.764|   0.4| 0.979| 2.241| 1.868|-0.977|  0.95|-0.151|       b|       x|\n",
      "|-0.103| 0.411| 0.144| 1.454| 0.761| 0.122| 0.444| 0.334|       b|       y|\n",
      "| 1.494|-0.205| 0.313|-0.854|-2.553| 0.654| 0.864|-0.742|       d|       z|\n",
      "|  2.27|-1.454| 0.046|-0.187| 1.533| 1.469| 0.155| 0.378|       b|       y|\n",
      "|-0.888|-1.981|-0.348| 0.156|  1.23| 1.202|-0.387|-0.302|       b|       z|\n",
      "+------+------+------+------+------+------+------+------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clean old table\n",
    "spark.sql(\"DROP TABLE IF EXISTS hive_table PURGE\")\n",
    "\n",
    "hive_sql_cmd = hive_table_ddl + \" LOCATION '\" + os.path.join(storage_platform,\"my_data_parquet\") + \"'\"\n",
    "\n",
    "spark.sql(hive_sql_cmd)\n",
    "\n",
    "answer_df = spark.sql(\"select * from hive_table limit 5\")\n",
    "\n",
    "assert answer_df.first().X_1 == 1.624\n",
    "assert answer_df.first().X_8 == -0.761\n",
    "\n",
    "answer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|row_count|\n",
      "+---------+\n",
      "|     2000|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) as row_count from hive_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
